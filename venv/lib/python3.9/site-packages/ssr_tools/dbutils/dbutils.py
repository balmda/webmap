import os 
import pandas as pd 
import psycopg2
from sqlalchemy import create_engine
from string import punctuation
import jenkspy
from psycopg2 import sql
from openpyxl import Workbook, load_workbook
import logging
import numpy as np
from binascii import hexlify
from psycopg2.extras import execute_values
import geopandas as gpd
import time

from ssr_tools.gen_utilities.gen_utilities import GenUtilities

ba_company_path = '/Users/balmdale/Dropbox/Company/'
rs_company_path = '/Users/rebecca/Library/CloudStorage/Dropbox/professional/Consulting/SafeStreetsResearch/Company/'
js_company_path = '/Users/jessica/Library/CloudStorage/Dropbox/SafeStreetsResearch/Company/'
aws_cred = 'Admin_Gen_Mktg/Software/AWS/aws_credentials_python.txt'

class DBUtils(GenUtilities):
    def __init__(self, hard_con=False, host=None, db_name=None, user=None, password=None, port=None,
                 verbose=True, filename=__file__):
        """
        Args:
            hard_con (bool): Change True if you want to manually enter in the connection args. Otherwise this script will find the correct info. Defaults to False
            host (str): name of host.
            db_name (str): name of db.
            user (str): username. 
            password (str): password.
            port (str): db port.
        """
        # import general utilities 
        GenUtilities.__init__(self, filename=filename)

        if not hard_con:
            if os.path.exists(os.path.join(ba_company_path, aws_cred)):
                print('Getting AWS credentials via BA path')
                aws_cred_path = os.path.join(ba_company_path, aws_cred)
            elif os.path.exists(os.path.join(js_company_path, aws_cred)):
                print('Getting AWS credentials via JS path')
                aws_cred_path = os.path.join(js_company_path, aws_cred)
            elif os.path.exists(os.path.join(rs_company_path, aws_cred)):
                print('Getting AWS credentials via RS path')
                aws_cred_path = os.path.join(rs_company_path, aws_cred)
            else: 
                print('Did not get AWS credential.')
                choice = input('Well, do you want to manually enter in the file path to the AWS credential file? [y]es, [n]o')
                if choice.strip().lower() in ["y", "yes"]:
                    aws_cred_path = input("Okay, paste that file path here: ").strip()

            try: 
                with open(aws_cred_path) as f:
                    self.host = f.readline().rstrip('\n')
                    self.password = f.readline().rstrip('\n')
                    self.port = f.readline().rstrip('\n')
                    self.user = f.readline()
                    f.close
            except:
                raise ValueError('IDK whats going on but you are not extracting the necessary AWS credentials. Talk to Brian or try manually typing in the AWS credentials...and tell Brian.')
        else: 
            self.host = host
            self.password = password
            self.port = port
            self.user = user
        
        self.db_name = db_name

        self.verbose = verbose
        if self.verbose:
            print("Connecting to database")

        if self.host is None or self.db_name is None  is None:
            raise ValueError("host and db args are required")
        # test the connection first
        try:
            conn = psycopg2.connect(dbname=self.db_name, user=self.user, password=self.password, host=self.host, port=self.port)
        except psycopg2.Error as e:
            raise ValueError(f"Could not connect to database:\n{e}")
        conn.close()

        # make a engine
        self.engine = create_engine(f'postgresql://{self.user}:{self.password}@{self.host}:{self.port}/{self.db_name}')
        if self.verbose: print('DBUtils has been created')

    def get_conn(self): 
        return psycopg2.connect(dbname=self.db_name, user=self.user, password=self.password, host=self.host, port=self.port)

    def get_engine(self): 
        return create_engine(f'postgresql://{self.user}:{self.password}@{self.host}:{self.port}/{self.db_name}')
    
    def create_db_con(self, host=None, db_name=None, user=None, password=None, port=None):
        """
        This will evntually be used to help us run SQL queries within python.
        Args:
            host (str, optional): name of host. Defaults to 'ssrc-db1.c9lbv7c8ir6a.us-east-2.rds.amazonaws.com'.
            db_name (str, optional): name of db. Defaults to 'ssrc01'.
            user (str, optional): username. Defaults to 'postgres'.
            password (str, optional): password. Defaults to 'g.&whu1*Q(Ndsn,j'.
            port (str, optional): db port. Defaults to '5432'.
        """
        conn = psycopg2.connect(dbname=self.db_name, user=self.user, password=self.password, host=self.host, port=self.port)
        engine = create_engine(f'postgresql://{self.user}:{self.password}@{self.host}:{self.port}/{self.db_name}')
        return conn, engine

    def load_table(self, file_path, schema, table_name, sheet_name=0, index=None, header=0, sep=",",
                    header_names=None,skiprows=None, default_na=True, upload_method="copy",engine=None, conn=None,if_exists='fail', 
                    verbose=True, print_time=True):
        """
        function to simply load tabular data into in a postgresql database. 
        Args:
            file_path (String): path to the table you want to load. 
            schema (string): string to the schema you are loading to
            table_name (string): table name that will be coded to this table loaded to postgresql.
            sheet_name (int/string, optional): name of the sheet you want to load. Defaults to 0 AKA the main tab.
            index (string, optional): name of index col. Defaults to None.
            header (int, optional): what row is the header on. Defaults to 0.
            skiprows (int, optional): what rows do you want to exclude from the loading process. Defaults to None.
            upload_method (str, optional): _description_. Defaults to "copy".
            engine (string): right now is None and requires users to create an engine within their python script and use that as an arguement. 
        """
        if print_time: start_time = time.time()

        file_type = file_path.split('.')[-1]
        if file_type == 'csv':
            if index is not None:
                df = pd.read_csv(file_path, sep=sep, header=header, skiprows=skiprows, index_col=index)
            else:
                df = pd.read_csv(file_path, sep=sep, header=header, skiprows=skiprows)
        elif file_type in ['xls', 'xlsx']:
            if index is not None:
                df = pd.read_excel(file_path, header=header, skiprows=skiprows, sheet_name=sheet_name, index_col=index)
            else:
                df = pd.read_excel(file_path, header=header, skiprows=skiprows, sheet_name=sheet_name)
        elif file_type =='txt':
            if index is not None:
                df = pd.read_csv(file_path, sep=sep, header=header, skiprows=skiprows, index_col=index)
            else:
                df = pd.read_csv(file_path, sep=sep, header=header, skiprows=skiprows)
        else:
            raise TypeError("Unable to handle file type of this file {}".format(file_path))
        print("loaded data to df: " + file_path)
        print('cleaning column names')
        df = self.clean_df_cols(df)

        print("renamed columns")
        if verbose: 
            print(df.head())
            self.list_cols(df)
        
        if file_type == 'csv':
            if index is not None:
                df.head(0).to_sql(table_name, con=engine, schema=schema, if_exists=if_exists, index=True)
            else:
                df.head(0).to_sql(table_name, con=engine, schema=schema, if_exists=if_exists, index=False)

            if upload_method == "copy":
                print('loading table via cur.copy_expert()')
                conn = conn
                cur = conn.cursor()
                sql = "COPY %s FROM STDIN WITH CSV HEADER DELIMITER AS ','"
                file = open(file_path)
                table = schema + "." + table_name
                print('loading rows into:' + table)
                with conn.cursor() as cur:
                    cur.execute("truncate table " + table + ";")
                    cur.copy_expert(sql=sql % table, file=file)
                    conn.commit()
                    cur.close()
            else:
                table = schema + "." + table_name
                print('loading table via to_sql(): ' + table)
                df.to_sql(table_name, con=engine, schema=schema, if_exists=if_exists, index=True)
        elif file_type in ['xls', 'xlsx']:
            print('loading table via to_sql()')
            df.to_sql(table_name, con=engine, schema=schema, if_exists='replace')
            
        print("CSV was succesfuly loaded: " + file_path + " to table " + schema + "." + table_name)
        print("Name in postgres: " + schema + "." + table_name)
        if print_time: print(f"Time to load: {round(((time.time() - start_time)/60),2)} minutes.\n")

    def clean_df_cols(self, df, clean_index=True, other_chars_dict=None): 
        #TODO move to gen utilities
        """_summary_
        Args:
            df (string): name of pandas dataframe
            clean_index (bool, optional): do you need to also clean the index to remove leading numbers and other characters? Defaults to True.
            other_chars_dict (dict, optional): any other characters not typical. Defaults to None.
        """
        df = df.rename(columns = lambda x: str(x).lower().strip())
        replace_dict={i:'_' for i in list(punctuation)+[' ', '\n']}
        if other_chars_dict is not None:
            replace_dict = merged_dict(replace_dict, other_chars_dict)
        for k, v in  replace_dict.items():
            df = df.rename(columns = lambda x: str(x).replace(k, v))
        df = df.rename(columns = lambda x: '_'+str(x) if str(x)[0].isdigit() else str(x))
        if clean_index and df.index.name is not None:
            df.index = df.index.rename(df.index.name.lower().strip())
            for k, v in  replace_dict.items():
                df.index = df.index.rename(df.index.name.replace(k, v))
            if str(df.index.name)[0].isdigit():
                df.index = df.index.rename('_' + str(df.index.name))
        print("Column rename succesful")
        return df
    
    def gdf_to_postgis(self,gdf,table,schema=None,columns=None,geom="geom",id="p_key",
                       multi=True,keep_case=False,srid=None,conn=None,
                       overwrite=False,no_geom=False, verbose=False):
        """
        Credit goes to PyBNA. 
        https://github.com/tooledesign/pybna/blob/master/pybna/dbutils.py

        Saves a geopandas geodataframe to Postgis.

        Parameters
        ----------
        gdf : geopandas GeoDataFrame object
            the GeoDataFrame to save
        table : str
            the table name
        schema : str, optional
            the schema name (not necessary if schema is qualified in table name)
        columns : list of str, optional
            a list of columns to save (if empty, save all columns)
        geom : str, optional
            name to use for the geom column
        id : str, optional
            name to use for the id/primary key column (created if it doesn't match anything in columns)
        multi : bool, optional
            convert single to multi if mixed types are found
        keep_case : bool, optional
            prevents conversion of column names to lower case
        srid : int, optional
            the projection to use (if none inferred from data)
        conn : psycopg2 connection object, optional
            an open psycopg2 connection
        overwrite : bool, optional
            drops an existing table
        no_geom : bool, optional
            copies only the table without accompany geometries (or processes a non-geo table)
        """
        # process inputs
        if schema is None:
            schema, table = self.parse_table_name(table)
        if schema is None:
            raise ValueError("Schema must either be given explicitly or qualified in table name")
        transaction = True
        if conn is None:
            transaction = False
            conn = self.get_conn()
        if not keep_case:
            gdf.columns = [c.lower() for c in gdf.columns]
        if columns is None:
            columns = gdf.columns
        elif not keep_case:
            columns = [c.lower() for c in columns]
        if srid is None and not no_geom:
            srid = int(gdf.geometry.crs["init"].split(":")[1])
        if overwrite:
            self.drop_table(table,schema,conn)
        
        self.verbose = verbose
        if self.verbose: print('grabbing geoms')
        if no_geom:
            # remove a geom column  (if there is one)
            try:
                gdf.drop(gdf.geometry.name,axis=1,inplace=True)
            except:
                pass
        else:
            # get geom column type
            shapely_type = gdf.geom_type.unique()
            if len(shapely_type) > 1:
                if len(shapely_type) > 2:
                    raise ValueError("Can't process more than one geometry type")
                elif multi:
                    g1 = shapely_type[0]
                    g2 = shapely_type[1]
                    if g1 in ["Point","MultiPoint"] and g2 in ["Point","MultiPoint"]:
                        pass
                    elif g1 in ["LineString","MultiLineString"] and g2 in ["LineString","MultiLineString"]:
                        pass
                    elif g1 in ["Polygon","MultiPolygon"] and g2 in ["Polygon","MultiPolygon"]:
                        pass
                    else:
                        raise ValueError("Can't process more than one geometry type")
                else:
                    raise ValueError("Can't process more than one geometry type")
            else:
                multi = False

            shapely_type = shapely_type[0]
            if shapely_type == "Point":
                if multi:
                    geom_type = "multipoint"
                else:
                    geom_type = "point"
            elif shapely_type == "MultiPoint":
                geom_type = "multipoint"
            elif shapely_type == "LineString":
                if multi:
                    geom_type = "multilinestring"
                else:
                    geom_type = "linestring"
            elif shapely_type == "MultiLineString":
                geom_type = "multilinestring"
            elif shapely_type == "Polygon":
                if multi:
                    geom_type = "multipolygon"
                else:
                    geom_type = "polygon"
            elif shapely_type == "MultiPolygon":
                geom_type = "multipolygon"
            else:
                raise ValueError("Incompatible geometry type".format(shapely_type))

        # remove geom column and any columns that aren't in the gdf
        if self.verbose: print('getting columns adn setting up empty table structure')
        tmp_cols = list()
        for c in columns:
            if c in gdf.columns and c != geom:
                tmp_cols.append(c)
        columns = list(tmp_cols)
        del tmp_cols

        db_columns = list()
        types = list()
        if not no_geom:
            db_columns.append(geom)
            types.append("text")
        for c in columns:
            if not no_geom:
                if c == gdf.geometry.name:
                    continue
            dtype = "text"
            if gdf[c].dtype in (np.int64,np.uint64):
                dtype = "bigint"
            if gdf[c].dtype in (np.int8,np.int16,np.int32,np.uint8,np.uint16,np.uint32):
                dtype = "integer"
            if gdf[c].dtype in (np.float16,np.float32,np.float64):
                dtype = "float"
            if c.lower() == id.lower():
                dtype += " primary key"
            db_columns.append(c)
            types.append(dtype)
        columns_with_types = [sql.SQL(" ").join([sql.Identifier(k),sql.SQL(v)]) for k, v in zip(db_columns,types)]
        if not id in db_columns:
            columns_with_types.insert(0,sql.SQL(" ").join([sql.Identifier(id),sql.SQL("serial primary key")]))
        columns_sql = sql.SQL(",").join(columns_with_types)
        q = sql.SQL("CREATE TABLE {}.{} ({})").format(
            sql.Identifier(schema),
            sql.Identifier(table),
            columns_sql
            )
        conn = self.get_conn()
        cur = conn.cursor()
        cur.execute(q)

        # copy data over
        if self.verbose: print('copying over data')
        insert_sql = sql.SQL("INSERT INTO {}.{} ({})").format(
            sql.Identifier(schema),
            sql.Identifier(table),
            sql.SQL(",").join([sql.Identifier(c) for c in db_columns])
            )
        insert_sql = insert_sql.as_string(conn)
        insert_sql += " VALUES %s"

        # convert geoms to wkt
        if self.verbose: print('converting and assigning geom')
        if not no_geom:
            gdf["wkbs"] = gdf.geometry.apply(lambda x: x.wkb).apply(hexlify).str.decode("utf-8").str.upper()
            gdf = gdf.drop(gdf.geometry.name,axis=1)
            gdf = gdf.rename(columns={"wkbs": geom})

        execute_values(cur,insert_sql,gdf[db_columns].values)
        if not no_geom:
            subs = {
                "schema": sql.Identifier(schema),
                "table": sql.Identifier(table),
                "geom": sql.Identifier(geom),
                "geom_type": sql.SQL(geom_type),
                "srid": sql.Literal(srid),
                "index": sql.Identifier("sidx_"+table)
            }
            if multi:
                q = sql.SQL(" \
                    ALTER TABLE {schema}.{table} ALTER COLUMN {geom} TYPE geometry({geom_type},{srid}) \
                    USING ST_Multi(ST_SetSRID({geom}::geometry,{srid})); \
                    CREATE INDEX {index} ON {schema}.{table} USING GIST ({geom}); \
                    ANALYZE {schema}.{table};"
                ).format(**subs)
            else:
                q = sql.SQL(" \
                    ALTER TABLE {schema}.{table} ALTER COLUMN {geom} TYPE geometry({geom_type},{srid}) \
                    USING ST_SetSRID({geom}::geometry,{srid}); \
                    CREATE INDEX {index} ON {schema}.{table} USING GIST ({geom}); \
                    ANALYZE {schema}.{table};"
                ).format(**subs)
            cur.execute(q)
            cur.close()
        if not transaction:
            conn.commit()
            conn.close()
    
    def get_schema(self,table):
        conn = self.get_conn()
        cur = conn.cursor()
        cur.execute(" \
            select nspname::text \
            from pg_namespace n, pg_class c \
            where n.oid = c.relnamespace \
            and c.oid = '%s'::regclass \
            " % table)
        return cur.fetchone()[0]
    
    def get_table_names(self, schema):
        """
        Get a list of all the tables in a schema

        Args:
            schema (str). A schema name. 

        """
        conn = self.create_db_con()[0]
        cur = conn.cursor() 
        if schema is None:
            raise ValueError('uhmmmm, you need to tell me the schema name...')

        subs = {"schema": sql.Literal(schema)}
        cur.execute(
            f"""
            SELECT table_name
            FROM information_schema.tables
            WHERE table_schema = '{schema}'
            """)
        return [x[0] for x in cur.fetchall()]
    
    def export_table(self,table,fpath,layer=None,geom="geom",pkey=None,nonspatial=False, verbose=False):
        """
        Credit goes to PyBNA. 
        https://github.com/tooledesign/pybna/blob/master/pybna/dbutils.py

        Exports the given table to a geopackage at the given path. Overwrites
        any pre-existing tables so use with caution!

        Args
        table : text
            the table in the database to export
        fpath : text
            the path to the geopackage file
        geom : text
            name of the geometry column
        pkey : text, optional
            the primary key column
        nonspatial : bool, optional
            if true, processes the table without spatial information
        """
        base, ext = os.path.splitext(fpath)
        # if not ext == ".gpkg":
        #     raise ValueError("Output file should be a geopackage (.gpkg)")

        if pkey is None:
            pkey = self.get_pkey(table)

        schema, table = self.parse_table_name(table)
        if schema is None:
            schema = self.get_schema(table)

        if layer is None:
            layer = table

        # set up check for list columns
        def is_iterable(ds):
            if isinstance(ds.iloc[0],(list,tuple,dict)):
                return True
            else:
                return False

        # load and export
        if self.verbose: print('connecting to db')
        conn = self.get_conn()
        if nonspatial:
            t = pd.read_sql(
                sql.SQL("select * from {}.{}").format(
                    sql.Identifier(schema),
                    sql.Identifier(table)
                ).as_string(conn),
                conn,
                index_col=pkey
                )
            if len(t) == 0:
                warnings.warn("Empty table {}.{}".format(schema,table))
            else:
                if self.verbose: print('Exporting non-spatial data')
                for col in t.columns:
                    if is_iterable(t[col]):
                        t[col] = t[col].astype("str")
                sqlite_conn = sqlite3.connect(fpath)
                t.to_sql(layer,sqlite_conn)
        else:
            if self.verbose: print('Exporting spatial data')
            t = gpd.read_postgis(
                sql.SQL("select * from {}.{}").format(
                    sql.Identifier(schema),
                    sql.Identifier(table)
                ).as_string(conn),
                conn,
                geom_col=geom,
                index_col=pkey
                )
            if len(t) == 0:
                warnings.warn("Empty table {}.{}".format(schema,table))
            else:
                for col in t.columns:
                    if is_iterable(t[col]):
                        t[col] = t[col].astype("str")
                t.to_file(fpath,layer=layer,driver="GPKG")

    def parse_table_name(self, name):
        """
        Splits a schema-qualified table name into schema and table. If no schema
        is given this returns None for the schema and simply provides the table name.
        Args
            name (string): A schema-qualified table name
        Returns
            schema: string
            table: string
        """
        try:
            schema, table = name.split(".")
            return schema, table
        except:
            return None, name

    def df_to_postgres(self, df, temp_out_path, table_name, engine, conn, table_schema, upload_method="copy", if_exists='fail', verbose=False):
        """
        basic approach to load db to postgres database. Nothing fancy here. Exports df to csv then uses load_csv(). 
        Args:
            df (object): pandas dataframe
            temp_out_path (string): temporary path tp export csv. csv will be deleted after it's loaded to database.
            table_name (string): name assigned to the table loaded to postgres. 
            engine (object): engine to connect to db. 
            table_schema (string): db schema (aka project name (e.g. p010, p006)
            if_exists (str, optional): what to do if the table already exists in db. possible values include: fail, replace, append. Defaults to 'fail'.
        """
        # Prepare data
        temp_file = os.path.join(temp_out_path, table_name + '.csv')
        df.to_csv(temp_file, header=True, index=False)
        self.load_table(file_path=temp_file, 
                schema=table_schema, 
                table_name=table_name, 
                if_exists=if_exists,
                upload_method=upload_method, 
                engine=engine, 
                conn=conn, 
                verbose=verbose
                )
        if(os.path.exists(temp_file) and os.path.isfile(temp_file)):
            os.remove(temp_file)
            print('temp csv successfully deleted: {}'.format(temp_file))
        else:
            print('temp csv was not found: {}'.format(temp_file))

    def create_new_index(self, df, index_name, ex_index=False, ex_col=None): 
        """
        quick func to add a new index to a dataframe
        Args:
            df: dataframe
            index_name (string): new of new index col
            ex_index (boolean, optional): is there an xisting index that needs to be replaced? Defaults to False
            ex_col (string/list, optional): column or list of columns to be used in the index. Defaults to None.
        """
        if ex_col == None: 
            df[index_name] = range(1, len(df) + 1)
            df.set_index(index_name, inplace=True)
            return df
        else:
            df.set_index([[index_name]], inplace=True)
            return df

    def list_cols(self, df):
        "basic function to print out columns"
        row = 0
        for i in list(df): 
            print(str(row) + ': ' + i)
            row += 1

    def jenks_bin(self, df, col, n_breaks):
        """
        Generates jenks breaks for a column and joins label to df.
        Args:
            df (pd.DataFrame): DataFrame to pass through
            col (string): input column
            n_breaks (string): number of breaks.
        """
        #TODO add optional return of break index (i.e. 1, 2, 3, etc.) for each bin
        if type(col) is not pd.Series:
            raise TypeError()
        breaks = jenkspy.jenks_breaks(col, n_classes=n_breaks)
        out = pd.cut(col, bins=breaks)

        out = out.astype(str)
        char_to_replace = {
                    '(': '',
                    ']': '',
                    ',': ' -'}
        for k, v in char_to_replace.items():
            out = out.str.replace(k, v)
        
        out = out.to_frame()
        out.columns = [str(col) + '_jenks_bin' for col in out.columns]
        df = df.drop(list(out.columns), axis=1, errors='ignore')
        df = df.join(out)

        return df

    def make_dir(self, parent_dir, new_dir):
        """
        Create a new direction (AKA folder) given a path and name of the nw directory.
        Args:
            parent_dir (string): path to where you want the directory to be created. 
            new_dir (string): name of the directory to be created. 
        """
        path = os.path.join(parent_dir, new_dir)
        try:
            os.makedirs(path, exist_ok = True)
            print(f'Directory {new_dir} created successfully')
        except OSError as error:
            print(f'Directory {new_dir} can not be created')

    def get_pkey(self, table, schema=None):
        """
        funtion to pull the primary key. 
        args:
            table (string): schema qualified table name. Doesn't have to be schema qualified if given a Schema arg.
            schema (string, optional): name of schema if not included in table arg. 
        """
        # connect to pg and read id col
        conn = self.create_db_con()[0]
        cur = conn.cursor()  

        if schema is not None:
            table = sql.Identifier(schema).as_string(conn)+"."+sql.Identifier(table).as_string(conn)
        else:
            schema, table = self.parse_table_name(table)
            if schema is None:
                table = sql.Identifier(table).as_string(conn)
            else:
                table = sql.Identifier(schema).as_string(conn)+"."+sql.Identifier(table).as_string(conn)

        q = sql.SQL(""" 
            SELECT a.attname 
            FROM   pg_index i 
            JOIN   pg_attribute a ON a.attrelid = i.indrelid 
                    AND a.attnum = ANY(i.indkey) 
            WHERE  i.indrelid = {}::regclass 
            AND    i.indisprimary;""").format(sql.Literal(table))
        cur.execute(q)

        if cur.rowcount == 0:
            raise ValueError("No primary key found for table %s" % table)
        elif cur.rowcount == 1:
            row = cur.fetchone()
            cur.close()
            conn.close()
            return row[0]
        else:
            row = cur.fetchall()
            cur.close()
            conn.close()
            return [x[0] for x in row]
    
    def does_table_exists(self,table,schema=None):
        """
        """
        conn = self.create_db_con()[0]
        cur = conn.cursor() 
        if schema is None:
            schema, table = self.parse_table_name(table)
        if schema is None:
            cur.execute(f"""
                SELECT 1 
                FROM information_schema.tables 
                WHERE table_name= '{table}'
                ;""")
        else:
            cur.execute(f"""
                SELECT 1 
                FROM information_schema.tables 
                WHERE table_schema= '{schema}'
                AND table_name= '{table}'
                ;""")
        if cur.rowcount == 0:
            print('table does not exists')
            return False
        else:
            print('table exists')
            return True
            
    def does_column_exists(self, table,column,schema=None):
        """
        """
        conn = self.create_db_con()[0]
        cur = conn.cursor() 
        if schema is None:
            schema, table = self.parse_table_name(table)
        assert (self.does_table_exists(table,schema=schema)), "Table {}.{} not found".format(schema,table)

        if schema is None:
            qstr = sql.SQL("""
                SELECT 1 
                FROM information_schema.columns 
                WHRE table_name={table} 
                AND column_name={column}""".format(table=sql.Literal(table), column=sql.Literal(column)))
            cur.execute(qstr)
        else:
            qstr = sql.SQL("""
                SELECT 1 
                FROM information_schema.columns 
                WHERE table_schema={schema} 
                AND table_name={table} 
                AND column_name={column}""".format(schema=sql.Literal(schema),table=sql.Literal(table), column=sql.Literal(column)))
            # cur = execute(qstr)
            cur.execute(qstr)
        if cur.rowcount == 0:
            return False
        else:
            return True
        
    def get_column_type(self,table, column, schema=None):
        """
        """
        conn = self.create_db_con()[0]
        cur = conn.cursor() 
        if schema is None:
            schema, table = self.parse_table_name(table)
        # assert (self.does_column_exists(table=table,column=column,schema=schema)), "Column {} not found in table {}".format(column,table)
        if schema is not None:
            schema_table = schema + '.' + table
        else:
            schema_table = table
        
        qstr = sql.SQL("""
                SELECT pg_catalog.format_type(a.atttypid,a.atttypmod)
                FROM   pg_catalog.pg_attribute a
                WHERE  a.attnum>0
                AND NOT a.attisdropped
                AND a.attrelid = {schema_table}::regclass
                AND a.attname = {column}
            """).format(column=sql.Literal(column),schema_table=sql.Literal(schema_table))
        cur.execute(qstr)

        row = cur.fetchone()
        return row[0]

    def find_srid_sql(self, table ,geom='geom', schema=None):
        if schema is None:
            schema, table = self.parse_table_name(table)
        conn = self.create_db_con()[0]

        q = f"""SELECT DISTINCT ST_SRID({geom}) 
                FROM {schema}.{table} 
                WHERE {geom} IS NOT NULL 
                LIMIT 1;
            """
            
        cur = conn.cursor()
        cur.execute(q)
        if cur.rowcount == 0:
            raise ValueError(f'Could not find SRID for: {schema}.{table}')
        return cur.fetchone()[0]
    
    def get_geom_column(self, table,schema=None,geom_types=None):
        """
        Returns the name of the first geometry column found. If geom_type is given,
        limit results to the given geometry type.

        Args:
            table (string), required A table name
            schema (string, optional) A schema name
            geom_types: A geometry type or list of geometry types

        """
        if schema is None:
            schema, table = self.parse_table_name(table)
        if schema is None:
            schema = self.get_schema(table)
        assert (self.does_table_exists(table,schema=schema)), "Table {}.{} not found".format(schema,table)
        if geom_types:
            geom_types = set(geom_types)
            assert (geom_types & GEOM_TYPES)

        conn = self.create_db_con()[0]
        cur = conn.cursor() 

        cur.execute(
            """
                select
                    gc.f_geometry_column::text,
                    gc.type::text
                from
                    public.geometry_columns gc,
                    pg_catalog.pg_class as c,
                    pg_catalog.pg_namespace as ns
                where
                    c.relname = '{table}'
                    and gc.f_table_schema = ns.nspname
                    and gc.f_table_name = c.relname
            """.format(table=sql.Literal(schema+"."+table))
            )
        if cur.rowcount == 0:
            return None
        if geom_types:
            for row in cur:
                if row[1].lower() in geom_types:
                    return row[0]
        else:
            return cur.fetchone()[0]
            
    def get_geom_type(self, table,schema=None,geom=None):
        """
        Returns the type of the geometry column

        Args:
            table : string, required. A table name
            schema : string, optional. A schema name
            geom: Name of the geom column

        """
        if schema is None:
            schema, table = self.parse_table_name(table)
        if schema is None:
            schema = self.get_schema(table)
        assert (self.does_table_exists(table,schema=schema)), "Table {}.{} not found".format(schema,table)

        if geom is None:
            geom = self.get_geom_column(table,schema)
        assert (geom is not None), "No geometry column found"
        
        conn = self.create_db_con()[0]
        cur = conn.cursor() 
        
        cur.execute(
            """
                select distinct geometrytype({geom})
                from {schema}.{table}
            """.format(schema=schema, table=table, geom=geom)
        )
        assert (cur.rowcount == 1), "More than one geometry type identified"
        return cur.fetchone()[0]


    def string_to_list(self, col, sql_type = None):
        """
        converts a string to a list. will also output to a sql expression.
        Args:
            col (string): column to be converted 
            sep (string, optional): type of character to be used to seperate values. Defaults to ','
            sql_type (string, optional): type of SQL object. Defaults to None.
        """
        if isinstance(col, list) and sql_type is None:
            return col
        if isinstance(col, str):
            new_col = [col]
        elif not isinstance(col, list):
            raise ValueError(f'{col} needs to be a string.')
        if sql_type != None:
            if sql_type == 'SQL':
                new_col = [sql.SQL(x) for x in col]
            elif sql_type == 'Identifier':
                new_col = [sql.Identifier(x) for x in col]
            elif sql_type == 'Literal':
                new_col = [sql.Literal(x) for x in col]
            elif sql_type == 'Placeholder':
                new_col = [sql.Placeholder(x) for x in col]
            else:
                raise TypeError(f"SQL type ({sql_type}) is raising an error. Does not match possible ypes: 'SQL', 'Identifier', 'Literal', or 'Placeholder'")
        else: ValueError(f'{col} was not assigned to new_col')
        return new_col

    def list_to_string(self, col, sep=','):
        """
        converts a list to a string.
        Args:
            col (string): column to be converted 
            sep (string, optional): type of character to be used to seperate values. Defaults to ','
        """
        if isinstance(col, str):
            new_col = col
        elif isinstance(col, list):
            new_col = sep.join(col)
        else:
            raise ValueError("Input needs to be a list.")
        return new_col


    def dict_merge(self, dict_1, dict_2):
        """mering dicts using an sing unpacking operator. Stack this is you need to merge more than two at a time.
        args:
            dict_1 (dict): dictionary 1 to be merged
            dict_2 (dict): dictionary 2 to be merged
        """
        if not isinstance(dict_1, dict):
            raise ValueError("dict_1 is not of dict type.")
        if not isinstance(dict_2, dict):
            raise ValueError("dict_2 is not of dict type.")
        merged_dict = {**dict_1, **dict_2}
        return merged_dict

    def col_list_to_sql_format(self, col, table=None):
        """Formats a string column(s) into a SQL composable
        args:
            col (string): column to convert to a sql object. 
            table (string, optional): table name if you need to reference the table. Defaults to None.  
        """
        if table is None:
            col_sql = [sql.Identifier(x) for x in self.string_to_list(col=col)]
        else:
            col_sql = [sql.Identifier(table, x) for x in self.string_to_list(col=col)]
        return sql.SQL(', ').join(col_sql)

    def make_sql_snippet(self, col, qstr, delimiter=', '):
        """
        Substitutes texts and iterates over a list of columns to produce a SQL snippet. Often times used to add new columns. 
        Args:
            col (string): column(s) to iterate over
            qstr (string): type of query
            delimiter (string, optional): how to separate each line in the query. Defaults to ', '
        """
        tmp = [qstr.replace("%s", x) for x in self.string_to_list(col=col)]
        snippet = [delimiter] * (len(tmp) * 2 - 1)
        snippet[0::2] = tmp
        snippet = ''.join(snippet)
        return snippet

    def execute_sql_script(self, file_name, db_name, dir_name='sql',subs=None, relative_path=True, commit_changes=False):
        """used to read file then feeds it through the execute function 
        https://stackoverflow.com/questions/19472922/reading-external-sql-script-in-python
        Executes the given script, applying subs if provided.
        Args:
            script (string): name of the SQL file. 
            db_name (string): name of db to connect to. 
            subs (dict, optional): Dict of SQL substitutions. Defaults to None. 
            commit_changes (bool, optional): option to commit changed to db. Defaults to False.
        """
        if relative_path:
            fpath = os.path.join(self.module_dir(), dir_name, file_name)
            print(fpath)
        else:
            fpath = file_name
        assert (os.path.isfile(fpath)), f"File {fpath} not found"
        f = open(fpath)
        query = f.read()
        f.close()
        return self.execute_sql(query=query,subs=subs,db_name=db_name, commit_changes=commit_changes)

    def execute_sql(self, query, db_name, subs=None, commit_changes=False):
        """used to actually execute the file or sql file 
        https://stackoverflow.com/questions/19472922/reading-external-sql-script-in-python
        Executes the given query, applying subs if provided.
        Args:
            query (string): _description_
            db_name (string): _description_
            subs (dict, optional): Dict of SQL substitutions. Defaults to None.
            commit_changes (bool, optional): option to commit changed to db. Defaults to False.
        """
        if not isinstance(query,sql.SQL) and not isinstance(query,sql.Composed):
            query = sql.SQL(query)
        conn = self.create_db_con(db_name)[0]
        cur = conn.cursor() 
        try:
            if subs:
                if not isinstance(subs, dict) and not None:
                    raise ValueError("subs is not of dict type.")
                else:
                    cur.execute(query.format(**subs))
            else:
                cur.execute(query)
        except Exception as e:
            conn.rollback()
            raise e
        if commit_changes:
            conn.commit()
            conn.close()
        else: 
            print('Results not committed')

    def compile_workbooks(self, workbooks_path, final_filename):
        """
        combine individual Excel workbooks into a single worksheet and saves it.

        Args:
            workbooks_path (string): filepath to excel files in single dir.
            final_filename (string): output file name (including '.xlsx')
        """
        if not isinstance(workbooks_path, str):
            raise TypeError("Argument workbooks_path must be of type str.")
        if not isinstance(final_filename, str):
            raise TypeError("Argument final_filename must be of type str.")
        if not os.path.exists(workbooks_path):
            raise NotADirectoryError("Argument workbook_path is not a directory.")
        if not final_filename.endswith(".xlsx"):
            raise ValueError('final_filename must end with the string ".xlsx"')
        if final_filename in os.listdir(workbooks_path):
            raise ValueError(f'There is already a file named {final_filename} in {workbooks_path}. '
                            f'Remove this file first or change the final_filename parameter value.')
        wbs = []
        for file in os.listdir(workbooks_path):
            if not file.startswith("~$") and file.endswith(".xlsx"):
                wb = load_workbook(os.path.join(workbooks_path, file))
                wbs.append(wb)
        
        final_wb = Workbook()
        final_ws = final_wb.worksheets[0]

        wb1 = wbs[0]
        ws1 = wb1.worksheets[0] 
    
        for j in range(1, ws1.max_column+1):
            final_ws.cell(row=1, column=j).value = ws1.cell(row=1, column=j).value

        current_row = 2

        for wb in wbs:
            for ws in wb.worksheets:
                mr = ws.max_row 
                mc = ws.max_column 

                for i in range (2, mr + 1): 
                    for j in range (1, mc + 1): 
                        current_cell = ws.cell(row = i, column = j) 
                        final_ws.cell(row = current_row, column = j).value = current_cell.value

                    current_row += 1

        final_wb.save(os.path.join(workbooks_path, final_filename))
    
    def drop_table(self, table, schema=None, conn=None, db_name=None, cascade=False, commit_changes=True):
        #NOTE untested
        """
        Function that drops a table given table and connection args.
        args
            table (string): name of the table (optionally with schema qualified, e.g. myschema.mytable)
            schema (string, optional): name of the schema (can't be used together with a schema-qualified table)
            conn (string, optional): a pre-existing connection (for rollback/transactions)
            cascade (bool, oopitonal): should this be a CASCADE drop?
        """
        if schema is None:
            schema, table = self.parse_table_name(table)
        if schema is None:
            raise ValueError('Need to provide a schema')
        if conn is None:
            if db_name is None: 
                conn = self.create_db_con(self.db_name)[0]
            else:
                conn = self.create_db_con(db_name)[0]
        if cascade:
            drop_qstr = sql.SQL(f"""DROP TABLE IF EXISTS {schema}.{table} CASCADE;""")
        else:
            drop_qstr = sql.SQL(f"""DROP TABLE IF EXISTS {schema}.{table};""")
        self.execute_sql(drop_qstr, db_name, commit_changes=True)
        if commit_changes:
            conn.commit()
            conn.close()
    
    def dict_not_empty(self, target_dict):
        if len(target_dict) > 0:
            return True
        else:
            return False
    
    def check_multi(self, table, schema=None, geom=None):
        """
        Returns true if the geometry column is a multi, false if it's not.

        Args:
            table -- name of the table (optionally with schema qualified, e.g. myschema.mytable)
            schema -- name of the schema (can't be used together with a schema-qualified table)
            geom -- name of the geom column. if not given, tries to guess. if multiple geom columns
                    are found uses the first one identified.
        """
        t = self.get_geom_type(table,schema,geom)
        if t.lower()[:5] == "multi":
            return True
        else:
            return False
    
    def get_aws_cred(self, hard_path=None):
        """Part of this will replace DBUTILS within ssr_tools
        This is specific to the SANDAG project and will be replaced with the import ssr_tools.dbutils.dbutils for future projects
        Args:
            hard_path (str, optional): If you need or want to feed in the actual file path to the AWS text file, do it here. Defaults to None.
            return_con (bool, optional): Do you need to connect to the db? If so, make this True. Defaults to False.
            db_name (str, optional): Name of database ONLY if you need to connect to it. Defaults to None.
        """
        if hard_path is None:
            if os.path.exists(os.path.join(ba_company_path, aws_cred)):
                print('Getting AWS credentials via BA path')
                aws_red_path = os.path.join(ba_company_path, aws_cred)
            elif os.path.exists(os.path.join(js_company_path, aws_cred)):
                print('Getting AWS credentials via JS path')
                aws_red_path = os.path.join(js_company_path, aws_cred)
            elif os.path.exists(os.path.join(rs_company_path, aws_cred)):
                print('Getting AWS credentials via RS path')
                aws_red_path = os.path.join(rs_company_path, aws_cred)
            else: 
                print('Did not get AWS credential.')
        else: 
            aws_red_path = hard_path

        with open(aws_red_path) as f:
            host = f.readline().rstrip('\n')
            password = f.readline().rstrip('\n')
            port = f.readline().rstrip('\n')
            user = f.readline()
            f.close
        
        return host, password, port, user

#test
# db = DBUtils(db_name='p025_sandag_atp')
# db = db.get_aws_cred()