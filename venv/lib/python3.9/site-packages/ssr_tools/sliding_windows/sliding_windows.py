import numpy as np
import os 
from ssr_tools.dbutils.dbutils import DBUtils 
from psycopg2 import sql

class SlidingWindows(DBUtils):
    """
    Object to conduct a sliding windows analysis. This class features four functions: 
        1) __init__: 
            This functions to connect to the database and to setup variables used throughout this analysis and other functions. 
        2) make_windows: 
            This function pulls in the network dataset then creates two different datasets. 
            A fully dissolved dataset unioned off a single or list of fields. 
            It then take takes that dissolved data and create windows using the window and step arguements. 
        3) prep_crash_data: 
            This functions does a lot. It creates a series of lists using the mode and injury severity used for aggregatin purposes and for addition fields onto the windows. 
            This functions also filters out crash records based on mode, severity, year, or a generic 'where' argument. 
        4) run_sliding_window_analysis: 
            This is primarily the crash aggregation onto the windows. 
    """

    def __init__(self, study_name, crash_table, network_table, crash_geom='geom', network_geom='geom',output_schema=None, verbose=True, deleted_temp_tables=False,
                host='ssrc-db2.c9lbv7c8ir6a.us-east-2.rds.amazonaws.com', db_name=None, user='postgres', password='g.&whu1*Q(Ndsn,j', port='5432'):
        """
        Initial function to connect to database and setup critical variables for analysis.
        Args:
            study_name (string): Name of study which will be the prefix of the output tables.
            crash_table (string): Name of the crash data stored in the postgres database. 
            network_table (string): Name of the crash data stored in the postgres database. 
            crash_geom (str, optional): Column name that store the geometry data for the crash data. Defaults to 'geom'.
            network_geom (str, optional): Column name that store the geometry data for the network data. Defaults to 'geom'.
            output_schema (string, optional): Where do you want this data exported to. Defaults to None.
            verbose (bool, optional): Want some outputs to tell you what the script is doing? Yes? Set to true. Else, set to false. Defaults to True.
            
            host (str, optional): postgres host string. Defaults to 'ssrc-db1.c9lbv7c8ir6a.us-east-2.rds.amazonaws.com'.
            db_name (string, optional): name of the database in which the data is stored and being exported to. Defaults to None.
            user (str, optional): postgres username. Defaults to 'postgres'.
            password (str, optional): _description_. postgres password to 'g.&whu1*Q(Ndsn,j'.
            port (str, optional): postgres port. Defaults to '5432'.
        """
        self.verbose = verbose
        DBUtils.__init__(self, host=host, db_name=db_name, user=user, password=password, port=port,
                         verbose=verbose, filename=__file__)
        
        if self.verbose: print("Setting up SlidingWindows object")
        #------------
        # Path and db Stuff
        self.dir = os.path.dirname(os.path.abspath(__file__))
        self.sql_dir = os.path.join(self.dir, 'sql')
        if db_name is None :
            raise ValueError("Need to have schema qualitified crash table") 

        if self.verbose: print('setting up basic vars')
        #------------
        # Name Stuff
        self.study_name = study_name
        self.network_out_name = study_name + '_output_network'
        self.short_name = study_name + '_short_window'
        self.sliding_name = study_name + '_long_window'
        self.crash_name = study_name + '_crash_table'
        self.exclude_name = study_name + '_dropped_crashes_table'

        #------------
        # Schema Stuff
        self.crash_schema, self.crash_table = self.parse_table_name(name=crash_table)
        if self.crash_schema is None :
            raise ValueError("Need to have schema qualitified crash table")
        if output_schema is None:
            raise ValueError("You need to provide an output schema.")
        else:
            self.output_schema = output_schema

        self.network_schema, self.network_table = self.parse_table_name(name=network_table)
        if self.network_schema is None :
            raise ValueError("Need to have schema qualitified network table")
        
        #------------
        # crash_table Stuff
        self.crash_table_pkey = self.get_pkey(self.crash_table, self.crash_schema)
        self.crash_table_geom = crash_geom
        self.crash_table_srid = self.find_srid_sql(table=self.crash_table, schema=self.crash_schema, geom = self.crash_table_geom)
        if self.verbose: print(f'{self.crash_table} has been loaded')
        if self.verbose: print(f'crash SRID: {self.crash_table_srid}')
        # network
        self.network_pkey = self.get_pkey(self.network_table, self.network_schema)
        self.network_geom = network_geom
        self.network_srid = self.find_srid_sql(table=self.network_table, schema=self.network_schema, geom = self.network_geom)
        if self.verbose: print(f'{self.network_table} has been loaded')
        if self.verbose: print(f'network_table SRID: {self.network_srid}')
        # check SRIDs to make sure they match
        if self.crash_table_srid != self.network_srid:
            raise ValueError(f"{self.crash_table} and {self.network_table} are not of the same SRID.\n{self.crash_table} SRID: {self.crash_table_geom}\n{self.network_table} SRID: {self.network_srid}")
        if self.verbose: print('Network and Crash Table SRIDs match!')
        
        #-------------
        # Assign above variables as SQL subs that will be used when ready and executing SQL files
        if self.verbose: print('setting up SQL substitions')
        self.sql_subs = {
            'network_schema': sql.SQL(self.network_schema),
            'network': sql.SQL(self.network_table),
            'network_pkey': sql.Identifier(self.network_pkey),
            'network_geom': sql.SQL(self.network_geom),
            'network_srid': sql.Literal(self.network_srid),
            'network_out_name': sql.Identifier(self.network_out_name),
            'network_out_name_raw': sql.SQL(self.network_out_name),
            
            'crash_schema': sql.SQL(self.crash_schema),
            'crash_table': sql.SQL(self.crash_table),
            'crash_name': sql.SQL(self.crash_name),
            'crash_table_pkey': sql.SQL(self.crash_table_pkey),
            'crash_table_geom': sql.SQL(self.crash_table_geom),
            
            'output_schema': sql.SQL(self.output_schema),
            'short_name': sql.SQL(self.short_name),
            'sliding_name': sql.SQL(self.sliding_name),
            'exclude_name': sql.SQL(self.exclude_name),

            'where_if': sql.SQL(''),
            'where_clause': sql.SQL('')
        }
        print('Alrighty, your SlidingWindows object has been initiated!')

    def make_windows(self, dissolve_col, window_length, step_length, where_clause = False, drop_dissolved_table=False):
        """ 
        Function to generate the sliding windows datasets:
            1) Base windows that are overlapping and use the the full window length and step size
            2) short non-overlapping windows which is often used as the final dataset for future analyses
        Args:
            dissolve_col (string/list): Name of the column(s) that will be used to dissolve the network.
            window_length (string): Length in SRID units that will be used to create the long windows. 
            step_length (string): Length in SRID units that will be used to slide each window along each corridor (general length is 0.1 * window_length). 
            where_clause (str, optional): Any query used to filter out features from the input network.
            
        """
        if self.verbose: print('-'*40, "\nCreating Sliding Windows")

        self.dissolve_col = dissolve_col
        self.step_length = step_length
        self.where_clause = where_clause
        self.sql_subs.update({'dissolved_network_fields': self.col_list_to_sql_format(dissolve_col)})

        dissolve_subs = self.dict_merge(self.sql_subs,{})

        if where_clause != False:
            dissolve_subs['where_if'] = sql.SQL("WHERE")
            dissolve_subs['where_clause'] = sql.SQL(where_clause)

        if self.verbose: print('dissolving network')
        self.execute_sql_script(
            file_name="dissolve_network.sql",
            subs=dissolve_subs,
            db_name=self.db_name,
            commit_changes=True
            )
        if self.verbose: print("Input network successfully dissolved on: " + str(self.dissolve_col))

        # Get the max number of times we'll need to iterate over the network
        # This drastically cuts down on processing time
        conn = self.create_db_con()[0]
        cur = conn.cursor()
        qstr = f"""SELECT CEILING(MAX(_length / {self.step_length}))::INT + 1 FROM {self.output_schema}.dissolved_network;"""
        cur.execute(qstr)
        max_iter = cur.fetchone()[0] 

        # get the column types of dissolve fields so they can be added to the short tables
        dissolve_cols_name_type = [x + ' ' + str(self.get_column_type(table=self.network_table, column=x, schema=self.network_schema)) for x in self.string_to_list(dissolve_col)]
        window_subs = self.dict_merge(self.sql_subs,{
            'max_iter': sql.Literal(max_iter),
            'window_length': sql.Literal(window_length),
            'step_length': sql.Literal(step_length),
            'dissolve_fields_list': sql.SQL(self.list_to_string(dissolve_col, ',')),
            'dissolve_fields_add': sql.SQL(self.list_to_string(dissolve_cols_name_type, ','))
        })

        if self.verbose: print('making windows')
        self.execute_sql_script(
            file_name="make_windows.sql",
            subs=window_subs,
            db_name=self.db_name,
            commit_changes=True
        )

        self.drop_dissolved_table = drop_dissolved_table
        if self.drop_dissolved_table: 
            self.drop_table(table = 'dissolved_network', schema=self.output_schema, conn=self.create_db_con()[0])
        
        if self.verbose: print('Windows have been processed!')
        if self.verbose: print('-'*40)

    def prep_crash_data(self, mode_col, sev_col, year = False, year_range = 5,
                            exclude_mode_col = False, exclude_sev_col = False,
                            where_clause = False):
        """ 
        Function to prep the crash data through filtering out crashes that are listed in the where_clause and preps the data by creating mode-severity fileds
        Args:
            mode_col (string): Name of the column that contains the primary crash mode.
            sev_col (string): Name of the column that contains the primary crash mode highest injury severity.
            year (int, optional): Most recent year to include. This is optional an only needed if you need to use a subset of the crash data that contains more recent crash data that is not needed. Defalts to False. 
            year_range (int, optional): Maximum number of years to be included in study. Defaults to 5 years. 
            exclude_mode_col (string, optional): Query to filter or specific modes. Defaults to False.
            exclude_sev_col (string, optional): Query to filter out specific injury severities. Defaults to False.
            where_clause (str, optional): Query to filter out any crashes. Defaults to false.
            
        """

        if self.verbose: print("Pre-processing crash_table")
        # mode variabls
        self.mode_col = mode_col
        self.exclude_mode_col = exclude_mode_col
        conn = self.create_db_con()[0]
        cur = conn.cursor() 
        qstr = f"""SELECT DISTINCT {self.mode_col} FROM {self.crash_schema}.{self.crash_table} ORDER BY {self.mode_col};"""
        cur.execute(qstr)
        self.mode_col_list = [x[0] for x in (cur.fetchall())]
        self.mode_col_list = [sub.replace('-', '_') for sub in self.mode_col_list]
        self.mode_col_list  = [sub.replace(' ', '_') for sub in self.mode_col_list]

        cur.close()
        conn.close()

        # severity variables
        self.sev_col = sev_col
        self.exclude_sev = exclude_sev_col
        conn = self.create_db_con()[0]
        cur = conn.cursor() 
        qstr = f"""SELECT DISTINCT {self.sev_col} FROM {self.crash_schema}.{self.crash_table} ORDER BY {self.sev_col};"""
        cur.execute(qstr)
        self.sev_col_list = [x[0] for x in (cur.fetchall())]
        self.sev_col_list = [sub.replace('-', '_') for sub in self.sev_col_list]
        self.sev_col_list  = [sub.replace(' ', '_') for sub in self.sev_col_list]
        cur.close()
        conn.close()

        # Add some values to the master subs list
        self.sql_subs.update({
            'crash_table_mode_col': sql.SQL(self.mode_col),
            'crash_table_sev_col': sql.SQL(self.sev_col)})

        crash_subs = self.dict_merge(self.sql_subs,{
            'where_flag': sql.SQL(''),
            'where_flag_add': sql.SQL(''),
            'where_flag_code': sql.SQL(''),

            'year_flag': sql.SQL(''),
            'year_flag_add': sql.SQL(''),
            'year_flag_code': sql.SQL(''),
            # Study Area
            'from_if': sql.SQL(''),

            'exclude_mode_col': sql.SQL(''),
            'exclude_mode_col_add': sql.SQL(''),
            'exclude_mode_col_code': sql.SQL(''),

            'exclude_sev_col': sql.SQL(''),
            'exclude_sev_col_add': sql.SQL(''),
            'exclude_sev_col_code': sql.SQL('')
        })

        if where_clause != False:
            crash_subs['where_flag'] = sql.SQL(' OR where_flag')
            crash_subs['where_flag_add'] = sql.SQL(", ADD IF NOT EXISTS where_flag BOOLEAN")
            crash_subs['where_flag_code'] = sql.SQL(f""",
                where_flag = CASE
                    WHEN {where_clause}
                    THEN FALSE
                    ELSE TRUE
                END
                """)

        if year != False:
            crash_subs['year_flag'] = sql.SQL(' OR year_flag')
            crash_subs['year_flag_add'] = sql.SQL(", ADD IF NOT EXISTS year_flag BOOLEAN")
            conn = self.create_db_con()[0]
            cur = conn.cursor() 
            qstr = f"""SELECT MAX({year}) FROM {self.crash_schema}.{self.crash_table};"""
            cur.execute(qstr)
            max_year = cur.fetchone()[0]
            crash_subs['year_flag_code'] = sql.SQL(f""",
                year_flag = CASE
                    WHEN t1.{year} <= {max_year} - {year_range}
                    THEN TRUE
                    ELSE FALSE
                END
                """)
            cur.close()
            conn.close()

        if exclude_mode_col != False:
            if self.verbose: 
                print("Excluding crash mode_col(s): " + ', '.join(self.string_to_list(exclude_mode_col)))
            conn = self.create_db_con()[0]
            crash_subs['exclude_mode_col_flag'] = sql.SQL(' OR mode_col_exclude_flag')
            crash_subs['exclude_mode_col_add'] = sql.SQL(", ADD IF NOT EXISTS mode_col_exclude_flag BOOLEAN")
            exclude_mode_col_list = sql.SQL(', ').join(self.string_to_list(exclude_mode_col, sql_type='Literal'))
            crash_subs['exclude_mode_col_code'] = sql.SQL(""",
                mode_col_exclude_flag = CASE
                    WHEN t1.{m} IN ({e})
                    THEN TRUE
                    ELSE FALSE
                END
            """.format(
                m = mode_col,
                e = exclude_mode_col_list.as_string(conn)))
            mode_col_list = [mode_col_list.remove(x) for x in self.string_to_list(exclude_mode_col)]
            self.mode_col_list = mode_col_list
            conn.close()
        
        if exclude_sev_col != False:
            if self.verbose: print("Excluding crash sev_colerity(s): " + ', '.join(self.string_to_list(exclude_sev_col)))
            conn = self.create_db_con()[0]
            crash_subs['exclude_sev_col_flag'] = sql.SQL(' OR sev_col_exclude_flag')
            crash_subs['exclude_sev_col_add'] = sql.SQL(", ADD IF NOT EXISTS sev_col_exclude_flag BOOLEAN")
            exclude_sev_col_list = sql.SQL(', ').join(self.string_to_list(exclude_sev_col, sql_type='Literal'))
            crash_subs['exclude_sev_col_code'] = sql.SQL(""",
                sev_col_exclude_flag = CASE
                    WHEN t1.{s} IN ({e})
                    THEN TRUE
                    ELSE FALSE
                END""".format(
                    s = sev_col, 
                    e = exclude_sev_col_list.as_string(conn)))
            sev_col_list = [sev_col_list.remove(x) for x in self.string_to_list(exclude_sev_col)]
            self.sev_col_list = sev_col_list
            conn.close()
        if self.verbose: print('cleaning crashes')
        self.execute_sql_script(
            file_name="clean_crashes.sql",
            db_name=self.db_name,
            subs=crash_subs,
            commit_changes=True)
        
        if self.verbose: print("Crash tables have been processed!")

    def run_sliding_window_analysis(self, weights = False, search_distance = 100):
        """
        Function conduct the crash data aggregation onto windows
        Args:
            weights (dict, optional): A dictionary to contains the injury severity-based weights. Defaults to Flase.
            search_distance (int, optional): Distance (buffer) to be used to aggregate crashes onto windows. Defaults to 100. 
        """
        score_cols = ['_'.join([m, 'score']) for m in self.mode_col_list]
        score_cols = [sub.replace('-', '_') for sub in score_cols]
        score_cols  = [sub.replace(' ', '_') for sub in score_cols]
        
        mode_col_sev_col_cols = ['_'.join([m, s]) for m in self.mode_col_list for s in self.sev_col_list]
        mode_col_sev_col_cols = [sub.replace('-', '_') for sub in mode_col_sev_col_cols]
        mode_col_sev_col_cols  = [sub.replace(' ', '_') for sub in mode_col_sev_col_cols]

        # Short windows score cols
        conn = self.create_db_con()[0]
        cur = conn.cursor() 
        add_cols_score = self.make_sql_snippet(score_cols , 'ADD COLUMN IF NOT EXISTS %s INT DEFAULT 0', ',')
        qstr = f"""ALTER TABLE {self.output_schema}.{self.short_name} {add_cols_score}""";
        cur.execute(qstr)
        conn.commit()
        cur.close()
        conn.close()
        
        # Sliding windows needs scores and mode_col sev_col cols
        conn = self.create_db_con()[0]
        cur = conn.cursor() 
        add_cols_score_mode = self.make_sql_snippet(score_cols + mode_col_sev_col_cols , 'ADD COLUMN IF NOT EXISTS %s INT DEFAULT 0', ',')
        qstr = f"""ALTER TABLE {self.output_schema}.{self.sliding_name} {add_cols_score_mode};"""
        cur.execute(qstr)
        conn.commit()
        cur.close()
        conn.close()

        if self.verbose: print("Added columns to sliding windows tables")
        # Do the analysis in a tmp file
        count_crash_subs = self.dict_merge(self.sql_subs,{
            'crash_table_mode_col': sql.SQL(self.mode_col),
            'crash_table_sev_col': sql.SQL(self.sev_col),
            'search_distance': sql.Literal(search_distance)})

        if self.verbose: print('aggregating crashes to windows')
        self.execute_sql_script(
            file_name="count_crashes.sql",
            db_name=self.db_name,
            subs=count_crash_subs,
            commit_changes=True)
        
        if self.verbose: print('crashes successfully aggregated to windows')
        
        # Get list of crash_table present
        conn = self.create_db_con()[0]
        cur = conn.cursor() 
        qstr = f"""SELECT DISTINCT {self.mode_col}_{self.sev_col} FROM {self.output_schema}.window_crash_raw WHERE {self.mode_col}_{self.sev_col} IS NOT NULL;"""
        cur.execute(qstr)
        tmp = cur.fetchall()
        cur.close()
        conn.close()
        crash_table_present = [i[0] for i in tmp]
        # Iterate through each crash type
        for c in crash_table_present:
            update_crash_counts_subs = self.dict_merge(self.sql_subs,{
                'crash_table_mode_col': sql.SQL(self.mode_col),
                'crash_table_sev_col': sql.SQL(self.sev_col),
                'crash_mode_col_sev_col': sql.SQL(c)})

            if self.verbose: print(f'aggregating crashes to windows for: {c}')
            self.execute_sql_script(
                file_name="update_crash_counts.sql",
                db_name=self.db_name,
                subs=update_crash_counts_subs,
                commit_changes=True)

        if self.verbose: print('applying crash weights')
        if weights != False:
            if any(isinstance(x, str) for x in list(weights.values())):
                raise ValueError("Crash weighting factors must be numeric")
            # check to make sure that there are no weight keys not in the data
            diff1 = np.setdiff1d(list(weights.keys()), self.sev_col_list)
            if diff1.size != 0:
                if self.verbose: 
                    print("FYI, not all crash sev_colerities in weights are present in crash data. The following sev_colerity(s) are not present: " + ", ".join(diff1))
            diff2 = np.setdiff1d(self.sev_col_list, list(weights.keys()))
            if diff2.size != 0:
                raise ValueError("Crash sev_colerity(s) unaccounted for in weights. Missing weight(s) for: "+ ", ".join(diff2))
        
        # Sum the scores by mode_col
        for m in self.mode_col_list:
            if weights != False:
                tmp = [m + '_' + x  + ' * ' + str(weights[x]) for x in weights]
            else:
                tmp = [m + '_' + x for x in self.sev_col_list]
            score_expression = self.make_sql_snippet(tmp, '%s', '+')
            calculate_scores_subs = self.dict_merge(self.sql_subs,{
                'm': sql.SQL(m),
                'score_expression': sql.SQL(score_expression)})
            self.execute_sql_script(
                file_name="calculate_scores.sql",
                db_name=self.db_name,
                subs=calculate_scores_subs,
                commit_changes=True)

        if self.verbose: print("All mode_cols scored")
        # assign the max scores to the short windows
        max_expression = self.make_sql_snippet(score_cols, 'MAX(t2.%s) AS %s', ',')
        update_expression = self.make_sql_snippet(score_cols, '%s = t2.%s', ',')
        where_expression = self.make_sql_snippet(self.string_to_list(self.dissolve_col), 'CASE WHEN t1.%s IS NULL AND t2.%s IS NULL THEN TRUE ELSE t1.%s = t2.%s END', ' AND ')
        update_short_windows_subs = self.dict_merge(self.sql_subs,{
            'max_expression': sql.SQL(max_expression),
            'update_expression': sql.SQL(update_expression),
            'where_expression': sql.SQL(where_expression)})

        if self.verbose: print('finalizing windows')
        self.execute_sql_script(
            file_name="update_short_windows.sql",
            db_name=self.db_name,
            subs=update_short_windows_subs,
            commit_changes=True)

        if self.verbose: print("Values score values applied to short windows")

    #TODO add function to delete all non-essential tables: deleted_temp_tables